{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3.3 [Supervised Neural Nets](https://courses.thinkful.com/data-201v1/project/4.3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters:\n",
    "\n",
    "1. Number of and complexity of _layers_ to include is determined by\n",
    "    * Computation Resources\n",
    "    * Cross Validation Searching for Convergence\n",
    "    \n",
    "2. _Alpha_ - neural networks use a regularization parameter that penalizes large coefficients (alpha scales with the penalty)\n",
    "3. _Activatoin Functions_ -determines whether the output from an individual perceptron is binary or continuous. \n",
    "    * **relu** is the binary fuction and most often used.  \n",
    "    * **sigmoid** is reasonable for continuous variables between _0 and 1_ allowing for more nuanced model (it does increase computation complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "artworks = pd.read_csv('https://media.githubusercontent.com/media/MuseumofModernArt/collection/master/Artworks.csv')\n",
    "\n",
    "# Select Columns.\n",
    "artworks = artworks[['Artist', 'Nationality', 'Gender', 'Date', 'Department',\n",
    "                    'DateAcquired', 'URL', 'ThumbnailURL', 'Height (cm)', 'Width (cm)']]\n",
    "\n",
    "# Convert URL's to booleans.\n",
    "artworks['URL'] = artworks['URL'].notnull()\n",
    "artworks['ThumbnailURL'] = artworks['ThumbnailURL'].notnull()\n",
    "\n",
    "# Drop films and some other tricky rows.\n",
    "artworks = artworks[artworks['Department']!='Film']\n",
    "artworks = artworks[artworks['Department']!='Media and Performance Art']\n",
    "artworks = artworks[artworks['Department']!='Fluxus Collection']\n",
    "\n",
    "# Drop missing data.\n",
    "artworks = artworks.dropna()\n",
    "\n",
    "# transform DateAcquired into datetimeobject\n",
    "artworks['DateAcquired'] = pd.to_datetime(artworks.DateAcquired)\n",
    "artworks['YearAcquired'] = artworks.DateAcquired.dt.year\n",
    "\n",
    "\n",
    "# Remove multiple nationalities, genders, and artists.\n",
    "artworks.loc[artworks['Gender'].str.contains('\\) \\('), 'Gender'] = '\\(multiple_persons\\)'\n",
    "artworks.loc[artworks['Nationality'].str.contains('\\) \\('), 'Nationality'] = '\\(multiple_nationalities\\)'\n",
    "artworks.loc[artworks['Artist'].str.contains(','), 'Artist'] = 'Multiple_Artists'\n",
    "\n",
    "\n",
    "# Convert dates to start date, cutting down number of distinct examples.\n",
    "artworks['Date'] = pd.Series(artworks.Date.str.extract(\n",
    "    '([0-9]{4})', expand=False))[:-1]\n",
    "\n",
    "# Final column drops and NA drop.\n",
    "sample = artworks.sample(50000, random_state=42)\n",
    "X = sample.drop(['Department', 'DateAcquired', 'Artist', 'Nationality', 'Date'], 1)\n",
    "\n",
    "# Create dummies separately.\n",
    "artists = pd.get_dummies(sample.Artist)\n",
    "nationalities = pd.get_dummies(sample.Nationality)\n",
    "dates = pd.get_dummies(sample.Date)\n",
    "\n",
    "# Concat with other variables, but artists slows this wayyyyy down so we'll keep it out for now\n",
    "X = pd.get_dummies(X, sparse=True)\n",
    "X = pd.concat([X, nationalities, dates], axis=1)\n",
    "\n",
    "Y = sample.Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Classify what department a piece of art should be in (5-way classification question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prints & Illustrated Books    0.5236\n",
       "Photography                   0.2309\n",
       "Architecture & Design         0.1112\n",
       "Drawings                      0.1010\n",
       "Painting & Sculpture          0.0333\n",
       "Name: Department, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish what a single 1000 perceptron layer looks like\n",
    "Not the best, the cross val score is below the majority class and there is a wide range of fold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Avg: 0.407\n",
      "Cross Val Std: 0.123\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model, with a single, 1000 perceptron layer.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different hidden layer sizes\n",
    "#### It's highly likely that the model could default to always guessing the majority class _Prints & Illustrated Books_   \n",
    "#### To do better than always guessing _Prints & Illustrated Books_ accuracy should be above 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many Small Layers:\n",
    "Better, the model's Accuracy Score average improved by 19 ppt and the standard deviation between folds is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 4.36 s, total: 40.8 s\n",
      "Wall time: 21.9 s\n",
      "Cross Val Avg: 0.598\n",
      "Cross Val Std: 0.044\n"
     ]
    }
   ],
   "source": [
    "# Many Small Layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10,10,10,10,10,10,10,10))\n",
    "%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few Large Layers\n",
    "Already with 3 layers of 1000 the nodel is taking > 10 minutes to fit 1 fold. I killed tha job to try with fewer layers\n",
    "Accuracy went up quite a bit but so did overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 18s, sys: 1min 40s, total: 9min 58s\n",
      "Wall time: 5min 8s\n",
      "Cross Val Avg: 0.559\n",
      "Cross Val Std: 0.041\n"
     ]
    }
   ],
   "source": [
    "# Many Small Layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000))\n",
    "%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Val Avg: 0.614\n",
      "Cross Val Std: 0.089\n"
     ]
    }
   ],
   "source": [
    "# Many Small Layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000, 1000))\n",
    "#%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will starting small and following up with a larger layer help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 6.99 s, total: 1min 35s\n",
      "Wall time: 49.7 s\n",
      "Cross Val Avg: 0.641\n",
      "Cross Val Std: 0.028\n"
     ]
    }
   ],
   "source": [
    "# Small then large\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,1000), random_state=33, )\n",
    "%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does it look like with the opposite? Start with a large layer and follow up with a smaller layer\n",
    "This is just guessing the majority class no bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 26.6 s, total: 2min 58s\n",
      "Wall time: 1min 33s\n",
      "Cross Val Avg: 0.517\n",
      "Cross Val Std: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Large then small This is guessing the majority class\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,10), random_state=33, )\n",
    "%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I'm curious about a handful of very small layers\n",
    "This is just guessing the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.53 s, sys: 577 ms, total: 4.1 s\n",
      "Wall time: 4.22 s\n",
      "Cross Val Avg: 0.517\n",
      "Cross Val Std: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Many Small Layer\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4, 3, 4, 2, 3), random_state=33, )\n",
    "%time mlp.fit(X, Y)\n",
    "\n",
    "cv_score = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(\"Cross Val Avg: %.3f\"%cv_score.mean())\n",
    "print(\"Cross Val Std: %.3f\"%np.std(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of thumb for layer size vs. depth\n",
    "* If more narrower layers the model could find complex patterns but also overfit\n",
    "* Deeper networks are harder to train, provides more opportunities for the model to find itself stuck at a local minimum rather than the global minimum - this issue is challenging to diagnose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
